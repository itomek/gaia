---
title: "Chat SDK"
description: "Build conversational AI applications with text chat, memory, and document Q&A capabilities"
---

<Note>
ðŸ“– **You are viewing:** User Guide - Learn how to build chat applications

**See also:** [SDK Reference](/sdk/sdks/chat) Â· [API Specification](/spec/chat-sdk)
</Note>

<Info>
  **Source Code:** [`src/gaia/chat/sdk.py`](https://github.com/amd/gaia/blob/main/src/gaia/chat/sdk.py) Â· [`src/gaia/rag/sdk.py`](https://github.com/amd/gaia/blob/main/src/gaia/rag/sdk.py)
</Info>

The GAIA Chat SDK provides a programmable interface for building text-based conversational AI applications with conversation memory and optional document Q&A (RAG).

<Info>
  **First time here?** Complete the [Setup](/setup) guide first to install GAIA and its dependencies.
</Info>

## Quick Start

<Steps>
  <Step title="Install dependencies">
    Activate your virtual environment and install GAIA with RAG support:

    ```bash
    uv pip install -e ".[rag]"
    ```
  </Step>

  <Step title="Create a simple chat">
    Start chatting with just a few lines of code:

    ```python title="simple_chat.py"
    from gaia.chat.sdk import SimpleChat

    chat = SimpleChat()
    response = chat.ask("What is Python?")
    print(response)

    # Follow-up with conversation memory
    response = chat.ask("Give me an example")
    print(response)
    ```
  </Step>

  <Step title="Use the full SDK">
    Access advanced features with the complete ChatSDK:

    ```python title="full_chat.py"
    from gaia.chat.sdk import ChatSDK, ChatConfig

    config = ChatConfig(
        show_stats=True,
        max_history_length=6
    )
    chat = ChatSDK(config)

    response = chat.send("Hello! My name is Alex.")
    print(response.text)

    response = chat.send("What's my name?")
    print(response.text)  # Will remember "Alex"
    ```
  </Step>
</Steps>

## Core Classes

### ChatConfig

Configure your chat session with these options:

```python
@dataclass
class ChatConfig:
    model: str = "Qwen3-Coder-30B-A3B-Instruct-GGUF"  # Default validated model
    max_tokens: int = 512
    system_prompt: Optional[str] = None
    max_history_length: int = 4
    show_stats: bool = False
    use_local_llm: bool = True
    assistant_name: str = "assistant"
```

### ChatResponse

Responses include text, history, and optional statistics:

```python
@dataclass
class ChatResponse:
    text: str
    history: Optional[List[str]] = None
    stats: Optional[Dict[str, Any]] = None
```

### SimpleChat

<Card title="SimpleChat" icon="comments">
  Lightweight wrapper for basic chat without complex configuration. Perfect for getting started quickly.
</Card>

## CLI Usage

### Interactive Mode

Start a conversational chat session:

<CodeGroup>
```bash Basic
# Start interactive chat
gaia chat
```

```bash With Statistics
# Show performance metrics
gaia chat --stats
```
</CodeGroup>

<Accordion title="Interactive Commands">
  - `/clear` - Clear conversation history
  - `/history` - Show conversation history
  - `/stats` - Show performance statistics
  - `/help` - Show help message
  - `quit`, `exit`, `bye` - End conversation
</Accordion>

### Single Query Mode

Execute one-shot queries:

```bash
# One-shot query
gaia chat "What is artificial intelligence?"

# With statistics
gaia chat --query "Hello" --show-stats
```

## Python SDK Examples

### Streaming Chat

Stream responses in real-time:

```python title="streaming.py"
from gaia.chat.sdk import ChatSDK

chat = ChatSDK()

print("AI: ", end="", flush=True)
for chunk in chat.send_stream("Tell me a story"):
    if not chunk.is_complete:
        print(chunk.text, end="", flush=True)
print()
```

### Custom Assistant Name

Personalize your assistant:

```python title="custom_assistant.py"
config = ChatConfig(
    assistant_name="Gaia",
    system_prompt="You are Gaia, a helpful AI assistant."
)
chat = ChatSDK(config)

response = chat.send("What's your name?")
print(f"Gaia: {response.text}")
```

### Session Management

Manage multiple conversation contexts:

```python title="sessions.py"
from gaia.chat.sdk import ChatSession

sessions = ChatSession()

# Create different contexts with custom names
work_chat = sessions.create_session(
    "work",
    system_prompt="You are a professional assistant.",
    assistant_name="WorkBot"
)

personal_chat = sessions.create_session(
    "personal",
    system_prompt="You are a friendly companion.",
    assistant_name="Buddy"
)

# Separate conversation histories
work_response = work_chat.send("Draft a team email")
personal_response = personal_chat.send("What's for dinner?")
```

### Conversation History

Access and manage conversation history:

```python title="history.py"
chat = ChatSDK()

chat.send("Hello")
chat.send("How are you?")

# Get formatted history
for entry in chat.get_formatted_history():
    print(f"{entry['role']}: {entry['message']}")

# Clear history
chat.clear_history()

# Check metrics
print(f"Conversation pairs: {chat.conversation_pairs}")
```

## Document Q&A (RAG)

<Note>
RAG (Retrieval-Augmented Generation) enables chatting with PDF documents using semantic search and context retrieval.
</Note>

### Python SDK with RAG

<Steps>
  <Step title="Enable RAG and index documents">
    ```python title="rag_chat.py"
    from gaia.chat.sdk import ChatSDK

    chat = ChatSDK()

    # Enable RAG and index documents
    chat.enable_rag(documents=["manual.pdf", "guide.pdf"])
    ```
  </Step>

  <Step title="Chat with document context">
    ```python
    # Chat with document context
    response = chat.send("What does the manual say about installation?")
    print(response.text)

    # Add more documents
    chat.add_document("troubleshooting.pdf")
    ```
  </Step>

  <Step title="Disable RAG when done">
    ```python
    # Disable RAG when done
    chat.disable_rag()
    ```
  </Step>
</Steps>

### CLI with RAG

<Tabs>
  <Tab title="Single Document">
    ```bash
    # Chat with single document
    gaia chat --index manual.pdf
    ```
  </Tab>

  <Tab title="Multiple Documents">
    ```bash
    # Chat with multiple documents
    gaia chat --index doc1.pdf doc2.pdf doc3.pdf
    ```
  </Tab>

  <Tab title="One-shot Query">
    ```bash
    # One-shot query with document
    gaia chat --index report.pdf --query "Summarize the key findings"
    ```
  </Tab>

  <Tab title="Voice Mode">
    ```bash
    # Voice with documents
    gaia talk --index manual.pdf
    ```
  </Tab>
</Tabs>

### RAG Configuration

Advanced RAG setup options:

```python title="advanced_rag.py"
# Advanced RAG setup
chat.enable_rag(
    documents=["doc1.pdf", "doc2.pdf"],
    chunk_size=600,      # Larger chunks for more context
    max_chunks=4,        # More chunks per query
    chunk_overlap=100,   # Overlap for context preservation
    show_stats=True
)

# Check indexed documents
if chat.rag:
    status = chat.rag.get_status()
    print(f"Indexed {status['indexed_files']} files")
    print(f"Total chunks: {status['total_chunks']}")
```

### RAG Debug Mode

Enable debug mode to see detailed retrieval information:

<CodeGroup>
```bash CLI Debug
# CLI with debug
gaia chat --index document.pdf --debug
```

```python Python Debug
# Python SDK with debug
from gaia.agents.chat.agent import ChatAgent

agent = ChatAgent(
    rag_documents=['document.pdf'],
    debug=True,
    silent_mode=False
)

result = agent.process_query("What is the vision statement?")
print(f"Debug trace saved to: {result['output_file']}")
```
</CodeGroup>

<Accordion title="Debug Information Includes">
  - Search keys generated by the LLM
  - Chunks found for each search
  - Relevance scores
  - Deduplication statistics
  - Score distributions
</Accordion>

### Chunking Strategies

<CardGroup cols={2}>
  <Card title="Structural Chunking" icon="bolt">
    **Default - Fast processing**

    ```python
    agent = ChatAgent(
        rag_documents=['document.pdf'],
        chunk_size=500,
        chunk_overlap=50
    )
    ```
  </Card>

  <Card title="LLM-Based Semantic" icon="brain">
    **More accurate context**

    ```python
    agent = ChatAgent(
        rag_documents=['document.pdf'],
        use_llm_chunking=True,
        chunk_size=500
    )
    ```
  </Card>
</CardGroup>

### RAG Troubleshooting

<AccordionGroup>
  <Accordion title="Missing Dependencies">
    ```bash
    uv pip install -e ".[rag]"
    ```
  </Accordion>

  <Accordion title="PDF Issues">
    - Ensure PDF has extractable text (not scanned images)
    - Check file is not password-protected
    - Verify file is not corrupted
  </Accordion>

  <Accordion title="Performance Tuning">
    ```python
    # Faster processing
    chat.enable_rag(documents=["doc.pdf"], chunk_size=300, max_chunks=2)

    # Better quality
    chat.enable_rag(documents=["doc.pdf"], chunk_size=600, max_chunks=5, chunk_overlap=100)

    # Memory efficient
    chat.enable_rag(documents=["doc.pdf"], chunk_size=400, max_chunks=2)
    ```
  </Accordion>
</AccordionGroup>

## API Reference

### ChatSDK Core Methods

<CardGroup cols={2}>
  <Card title="Messaging" icon="message">
    - `send(message: str) -> ChatResponse`
    - `send_stream(message: str)`
  </Card>

  <Card title="History" icon="clock-rotate-left">
    - `get_history() -> List[str]`
    - `clear_history()`
    - `get_formatted_history() -> List[Dict]`
  </Card>

  <Card title="RAG (Document Q&A)" icon="book">
    - `enable_rag(documents=None, **kwargs)`
    - `disable_rag()`
    - `add_document(path: str) -> bool`
  </Card>

  <Card title="Configuration" icon="gear">
    - `update_config(**kwargs)`
    - `get_stats() -> Dict`
    - `display_stats(stats=None)`
    - `start_interactive_session()`
  </Card>
</CardGroup>

### SimpleChat Methods

- `ask(question: str) -> str` - Ask question, get response
- `ask_stream(question: str)` - Ask question, stream response
- `clear_memory()` - Clear conversation memory
- `get_conversation() -> List[Dict]` - Get conversation history

### ChatSession Methods

- `create_session(id, config=None, **kwargs) -> ChatSDK` - Create new session
- `get_session(id) -> ChatSDK` - Get existing session
- `delete_session(id) -> bool` - Delete session
- `list_sessions() -> List[str]` - List all sessions
- `clear_all_sessions()` - Delete all sessions

### Utility Functions

- `quick_chat(message, system_prompt=None, model=None, assistant_name=None) -> str`
- `quick_chat_with_memory(messages, system_prompt=None, model=None, assistant_name=None) -> List[str]`

## Best Practices

<CardGroup cols={2}>
  <Card title="Choose the Right Interface" icon="code-branch">
    `SimpleChat` for basic needs, `ChatSDK` for full features, `ChatSession` for multi-context apps
  </Card>

  <Card title="Memory Management" icon="memory">
    Configure `max_history_length` based on memory constraints
  </Card>

  <Card title="Performance" icon="chart-line">
    Enable `show_stats=True` during development
  </Card>

  <Card title="Error Handling" icon="shield">
    Wrap chat operations in try-catch blocks
  </Card>

  <Card title="Resource Cleanup" icon="broom">
    Clear sessions when done to free memory
  </Card>

  <Card title="Assistant Naming" icon="tag">
    Use meaningful names for distinct use cases
  </Card>
</CardGroup>

## Next Steps

<CardGroup cols={2}>
  <Card title="Voice Interaction" icon="microphone" href="/guides/talk">
    Add speech recognition and text-to-speech
  </Card>

  <Card title="CLI Reference" icon="terminal" href="/reference/cli">
    Explore all command-line options
  </Card>

  <Card title="Development Guide" icon="code" href="/reference/dev">
    Learn to build custom agents
  </Card>

  <Card title="API Server" icon="server" href="/reference/api">
    Integrate via OpenAI-compatible API
  </Card>
</CardGroup>

---

<small style="color: #666;">

**License**

Copyright(C) 2024-2025 Advanced Micro Devices, Inc. All rights reserved.

SPDX-License-Identifier: MIT

</small>